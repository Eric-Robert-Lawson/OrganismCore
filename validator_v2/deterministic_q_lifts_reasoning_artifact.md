# üìã **DETERMINISTIC ‚Ñö-LIFTS REASONING ARTIFACT**

**Document**: `deterministic_q_lifts_reasoning_artifact.md`  
**Purpose**: Complete computational protocol for lifting all modular results to unconditional ‚Ñö-proofs  
**Status**: Action plan with complete scripts and reproducibility instructions  
**Date**: January 25, 2026  
**Author**: Eric Robert Lawson (OrganismCore Project)

---

**IMPORTANT TO NOTE THAT THE FIRST PART OF THE ARTIFACT IS MEANT AS A BASIS, UPDATES ARE THE ACTIONS TAKEN TO COMPUTE!**

## **üéØ OBJECTIVE**

Eliminate **all** reliance on rank-stability heuristics by producing deterministic certificates over ‚Ñö for:

1. **Rational Kernel-Basis Reconstruction** (707-dimensional cokernel over ‚Ñö)
2. **CP3 Rational Certificates** (variable-count barrier over ‚Ñö)
3. **SNF Intersection Matrix** (exact algebraic cycle rank)
4. **Data Integrity Checksums** (SHA-256 verification)
5. **Complete Reproducibility Instructions** (end-to-end protocol)

**Current Status**: We have **rank ‚â• 1883 over ‚Ñ§** (unconditional). This artifact provides the path to **complete ‚Ñö-certification**.

---

## **üìä STATUS SUMMARY**

| **Component** | **Current Status** | **Deterministic Target** | **Priority** | **Timeline** |
|---------------|-------------------|--------------------------|--------------|--------------|
| **Rank ‚â• 1883 over ‚Ñ§** | ‚úÖ **PROVEN** (k=1883 cert) | ‚úÖ Complete | ‚Äî | **Done** |
| **Dimension = 707 over ‚Ñö** | üìä Strong evidence (5-prime) | ‚ö†Ô∏è Rational basis needed | **High** | 1-2 weeks |
| **CP3 Barrier over ‚Ñö** | üìä Multi-prime (30,075 tests) | ‚ö†Ô∏è Rational cert sample | **Medium** | 1 week |
| **SNF (Cycle Rank)** | üìä Upper bound ‚â§12 (Shioda) | ‚ö†Ô∏è Exact rank needed | **Medium** | 2-4 weeks |
| **Data Checksums** | ‚ö†Ô∏è Partial (triplet JSONs) | ‚úÖ Complete manifest | **Low** | 1 day |
| **Reproducibility** | ‚úÖ Complete (reasoning artifacts) | ‚úÖ End-to-end protocol | **Low** | 1 day |

---

## **COMPANION PAPERS REFERENCE**

This reasoning artifact supports the following papers in the OrganismCore repository:

1. **`hodge_gap_cyclotomic.tex`** (validator/) ‚Äî 98.3% Gap paper
2. **`technical_note.tex`** (validator/) ‚Äî Information-Theoretic Obstruction paper
3. **`coordinate_transparency.tex`** (validator_v2/) ‚Äî Coordinate Transparency paper
4. **`variable_count_barrier.tex`** (validator_v2/) ‚Äî Variable-Count Barrier paper
5. **`4_obs_1_phenom.tex`** (validator_v2/) ‚Äî Four Obstructions Converge (synthesis paper)

**This artifact specifically provides deterministic ‚Ñö-certificates for claims in:**
- `hodge_gap_cyclotomic.tex` (dimension = 707 claim)
- `variable_count_barrier.tex` (CP3 coordinate collapse tests)
- `hodge_gap_cyclotomic.tex` (SNF for exact cycle rank)

---

## **PART 1: RATIONAL KERNEL-BASIS RECONSTRUCTION** üî¢

### **Goal**
Construct explicit 707-dimensional ‚Ñö-basis for $H^{2,2}_{\mathrm{prim,inv}}(V, \QQ)$ via CRT from modular kernel bases.

### **Current Status**
- ‚úÖ Modular kernel bases computed (mod 53, 79, 131, 157, 313)
- ‚úÖ Exact agreement: all 5 primes give 707-dimensional cokernel
- ‚úÖ Monomial correspondence verified (SHA-256 hash match)
- ‚ö†Ô∏è **Missing**: Rational coefficient reconstruction for kernel basis vectors

### **Why This Matters**
**For `hodge_gap_cyclotomic.tex`:**
- **Eliminates heuristic**: Currently dimension=707 relies on rank-stability principle
- **Provides explicit basis**: Each of 707 classes will have explicit ‚Ñö-coefficients
- **Upgrades Theorem 1.1**: From "strong evidence" to "unconditionally proven over ‚Ñö"

**For `4_obs_1_phenom.tex`:**
- **Validates foundation**: All four obstructions rest on dimension=707 claim
- **Enables further computation**: Rational basis needed for intersection pairings, period computation

---

### **METHOD: CRT + Rational Reconstruction for Kernel Vectors**

#### **Overview**
Each kernel basis vector $v_i$ (for $i=1,\ldots,707$) has coefficients in the monomial basis:
$$v_i = \sum_{j=1}^{1883} c_{ij} \cdot m_j$$
where $m_j$ are the 1883 weight-0 degree-18 monomials.

**Modular data**: We have $c_{ij} \in \FF_p$ for each prime $p \in \{53, 79, 131, 157, 313\}$.

**Goal**: Recover $c_{ij} \in \QQ$ via CRT + rational reconstruction.

---

#### **Algorithm**

generate kernals from inv files, must do this for each prime! requires the monomial and triplet jsons for each prime! These files should be ~18mb each when created.

```python
#!/usr/bin/env python3
"""
extract_kernel_from_triplets.py

Build a dense matrix from triplet JSON + monomials list and compute the kernel
(nullspace) over a prime p. Writes kernel_p{p}.json with modular kernel basis.

Usage (example):
  python3 validator_v2/extract_kernel_from_triplets.py \
    --triplet validator/saved_inv_p313_triplets.json \
    --monomials validator/saved_inv_p313_monomials18.json \
    --prime 313 \
    --out validator_v2/kernel_p313.json

Inputs:
  --triplet : triplet JSON (supports {"triplets": [[r,c,v],...]} or plain list)
  --monomials : monomials JSON (list of monomial strings) or a JSON with key "monomials"
  --prime : prime modulus (int)
  --out : output kernel JSON path (default kernel_p{p}.json)

Output format (JSON):
{
  "kernel": [
    [a_1, a_2, ..., a_n],   # vector 0 (coeffs mod p)
    ...
  ],
  "metadata": {
    "prime": p,
    "n_vectors": k,
    "n_coeffs": n,
    "matrix_dims": [m, n],
    "source_triplet": "...",
    "source_monomials": "..."
  }
}

Notes / assumptions:
 - The triplet 'col' index is assumed to index the same monomial ordering as the monomials file.
 - If triplet columns exceed number of monomials, columns are capped or a warning printed.
 - The code constructs a dense m x n matrix (m rows, n columns). For your typical sizes
   (e.g., m~2590, n~2016) this is feasible in memory (~40 MB for int64). If larger,
   consider a sparse approach.
 - Requires numpy installed.

Author: Assistant (for OrganismCore)
Date: 2026-01-25
"""
import argparse
import json
import math
import sys
from pathlib import Path

try:
    import numpy as np
except Exception as e:
    print("ERROR: numpy is required. Install with `pip install numpy`.", file=sys.stderr)
    raise

def load_triplets(path):
    with open(path, 'r') as f:
        data = json.load(f)
    # try common shapes
    if isinstance(data, dict):
        if 'triplets' in data:
            trip = data['triplets']
        elif 'matrix' in data:
            trip = data['matrix']
        else:
            # find first list-of-lists
            trip = None
            for v in data.values():
                if isinstance(v, list) and v and isinstance(v[0], list):
                    trip = v
                    break
            if trip is None:
                raise ValueError(f"Unrecognized triplet JSON structure in {path}")
    elif isinstance(data, list):
        trip = data
    else:
        raise ValueError(f"Unrecognized triplet JSON in {path}")
    normalized = []
    for t in trip:
        if isinstance(t, list) and len(t) >= 3:
            r, c, v = int(t[0]), int(t[1]), int(t[2])
            normalized.append((r, c, v))
        elif isinstance(t, dict) and {'row','col','val'}.issubset(t.keys()):
            normalized.append((int(t['row']), int(t['col']), int(t['val'])))
        else:
            raise ValueError("Triplet entry not understood: " + str(t))
    return normalized

def load_monomials(path):
    with open(path, 'r') as f:
        data = json.load(f)
    if isinstance(data, dict) and 'monomials' in data:
        mons = data['monomials']
    elif isinstance(data, list):
        mons = data
    else:
        raise ValueError(f"Unrecognized monomials file: {path}")
    return mons

def build_dense_matrix(triplets, ncols, dtype=np.int64):
    # infer nrows
    maxr = 0
    for r,c,v in triplets:
        if r > maxr:
            maxr = r
    nrows = maxr + 1
    A = np.zeros((nrows, ncols), dtype=dtype)
    for r,c,v in triplets:
        if 0 <= c < ncols:
            A[r, c] = (A[r, c] + int(v))
        else:
            # ignore out-of-range columns but warn once
            pass
    return A

def mod_reduce(A, p):
    A_mod = np.mod(A, p).astype(np.int64)
    return A_mod

def nullspace_modp(A_mod, p):
    """
    Compute nullspace of A over Z/pZ (A is m x n), return list of n-length vectors mod p.
    Uses row-reduction to RREF and builds nullspace basis (free columns = parameters).
    """
    A = A_mod.copy()
    m, n = A.shape
    A = A % p
    pivots = []
    row = 0
    # Convert to RREF
    for col in range(n):
        if row >= m:
            break
        # find pivot row
        pivot = None
        for r in range(row, m):
            if A[r, col] % p != 0:
                pivot = r
                break
        if pivot is None:
            continue
        # swap
        if pivot != row:
            A[[row, pivot], :] = A[[pivot, row], :]
        inv = pow(int(A[row, col]), -1, p)
        # normalize pivot row
        A[row, :] = (A[row, :] * inv) % p
        # eliminate other rows
        for r in range(m):
            if r == row:
                continue
            if A[r, col] != 0:
                factor = int(A[r, col])
                A[r, :] = (A[r, :] - factor * A[row, :]) % p
        pivots.append(col)
        row += 1
    pivots_set = set(pivots)
    free_cols = [c for c in range(n) if c not in pivots_set]
    basis = []
    # For each free variable, construct a nullspace vector
    # Suppose variables x_free = 1, others 0, then solve pivot rows
    # After RREF, pivot rows: x_pivot + sum_{free} A[row,free]*x_free = 0 => x_pivot = -A[row,free]
    for free_col in free_cols:
        vec = np.zeros(n, dtype=np.int64)
        vec[free_col] = 1
        # for each pivot row i, pivot_col = pivots[i], row i is the i-th pivot row
        for i, pivot_col in enumerate(pivots):
            # The row index in A for pivot i is i (since we built RREF with row increments)
            # But pivot rows may be fewer than m; safe to check bounds
            if i >= A.shape[0]:
                continue
            coeff = int(A[i, free_col]) % p
            if coeff != 0:
                vec[pivot_col] = (-coeff) % p
        basis.append(vec.tolist())
    return basis

def write_kernel_json(outpath, basis, prime, n_coeffs, n_rows, n_cols, triplet_path, monomials_path):
    out = {
        "kernel": basis,
        "metadata": {
            "prime": int(prime),
            "n_vectors": len(basis),
            "n_coeffs": int(n_coeffs),
            "matrix_dims": [int(n_rows), int(n_cols)],
            "source_triplet": str(triplet_path),
            "source_monomials": str(monomials_path)
        }
    }
    with open(outpath, 'w') as f:
        json.dump(out, f, indent=2)

def main():
    p = argparse.ArgumentParser()
    p.add_argument('--triplet', required=True, help='Path to triplet JSON')
    p.add_argument('--monomials', required=True, help='Path to monomials JSON (list)')
    p.add_argument('--prime', required=True, type=int, help='Prime modulus to reduce to')
    p.add_argument('--out', default=None, help='Output kernel JSON path (default: kernel_p{p}.json)')
    args = p.parse_args()

    triplet_path = Path(args.triplet)
    monomials_path = Path(args.monomials)
    prime = int(args.prime)
    outpath = Path(args.out) if args.out else Path(f"validator_v2/kernel_p{prime}.json")

    print(f"[+] Loading triplets from {triplet_path} ...")
    triplets = load_triplets(str(triplet_path))
    print(f"[+] {len(triplets)} triplets loaded")

    print(f"[+] Loading monomials from {monomials_path} ...")
    monomials = load_monomials(str(monomials_path))
    ncols = len(monomials)
    print(f"[+] {ncols} monomials")

    # Build dense matrix (rows inferred from triplets)
    print("[+] Building dense matrix (may use significant memory for large shapes)...")
    A = build_dense_matrix(triplets, ncols, dtype=np.int64)
    n_rows, n_cols = A.shape
    print(f"[+] Matrix shape: {n_rows} x {n_cols}")

    # Reduce mod p
    print(f"[+] Reducing matrix entries modulo {prime} ...")
    A_mod = mod_reduce(A, prime)

    # Compute nullspace over F_p
    print("[+] Computing nullspace over F_p (this may take some time)...")
    basis = nullspace_modp(A_mod, prime)
    k = len(basis)
    print(f"[+] Nullspace size (mod {prime}) = {k} vectors")

    # Write output
    print(f"[+] Writing kernel JSON to {outpath} ...")
    write_kernel_json(str(outpath), basis, prime, n_coeffs=n_cols, n_rows=n_rows, n_cols=n_cols,
                      triplet_path=triplet_path, monomials_path=monomials_path)
    print("[+] Done. Notes:")
    print("    - Each kernel vector is given as a list of integers modulo p (0..p-1).")
    print("    - If expected kernel dimension differs from produced, check monomial ordering and triplet column indices.")

if __name__ == "__main__":
    main()
```

then the rational_kernal_basis.py

```python
#!/usr/bin/env python3
"""
rational_kernel_basis.py (fixed verification)

Reconstruct rational kernel basis for H^{2,2}_{prim,inv}(V, Q) via CRT + rational
reconstruction.

Fixes:
 - Verifies reconstructed rational n/d against modular residues using the
   congruence n ‚â° r_p * d (mod p) rather than attempting to invert d mod p,
   which can fail when gcd(d,p) != 1.
 - Logs failed coefficient positions to a failures JSON file.
 - Robust output handling when some coefficients failed reconstruction.

Usage:
  python3 rational_kernel_basis.py \
    --kernels validator_v2/kernel_p53.json validator_v2/kernel_p79.json ... \
    --primes 53 79 131 157 313 \
    --out validator_v2/kernel_basis_Q.json \
    [--sample N] [--failures_out validator_v2/reconstruction_failures.json]

Author: Assistant (for OrganismCore)
Date: 2026-01-25 (fixed)
"""
import json
import math
import sys
from pathlib import Path

# Increase string conversion limits for large rationals (Python 3.11+)
try:
    sys.set_int_max_str_digits(10_000_000)
except AttributeError:
    pass

def iterative_crt(residues):
    x, M = residues[0][1], residues[0][0]
    for (m, r) in residues[1:]:
        inv = pow(M % m, -1, m)
        t = ((r - x) * inv) % m
        x = x + t * M
        M = M * m
        x %= M
    return x, M

def rational_reconstruction(a, m, bound=None):
    a = int(a) % int(m)
    m = int(m)
    if bound is None:
        bound = int(math.isqrt(m // 2))
    r0, r1 = m, a
    s0, s1 = 1, 0
    t0, t1 = 0, 1
    while r1 != 0 and abs(r1) > bound:
        q = r0 // r1
        r0, r1 = r1, r0 - q * r1
        s0, s1 = s1, s0 - q * s1
        t0, t1 = t1, t0 - q * t1
    if r1 == 0:
        return None
    num = r1
    den = t1
    if den == 0:
        return None
    if den < 0:
        num, den = -num, -den
    if abs(num) > bound or den > bound:
        return None
    if ((num - a * den) % m) != 0:
        return None
    g = math.gcd(abs(num), den)
    num //= g
    den //= g
    return (num, den)

def load_kernel_modp(path):
    with open(path) as f:
        data = json.load(f)
    if 'kernel' in data:
        return data['kernel']
    elif 'basis' in data:
        return data['basis']
    else:
        raise ValueError(f"No kernel/basis field in {path}")

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--kernels', nargs='+', required=True,
                        help='Paths to kernel_p*.json files (order: primes)')
    parser.add_argument('--primes', nargs='+', type=int, required=True,
                        help='Primes in same order as kernel files')
    parser.add_argument('--out', default='kernel_basis_Q.json',
                        help='Output file for rational basis')
    parser.add_argument('--sample', type=int, default=None,
                        help='Reconstruct only first N basis vectors (for testing)')
    parser.add_argument('--failures_out', default='validator_v2/reconstruction_failures.json',
                        help='JSON file to record failed coefficient positions')
    args = parser.parse_args()

    if len(args.kernels) != len(args.primes):
        print("ERROR: Number of kernel files must match number of primes")
        return

    print(f"[+] Loading {len(args.kernels)} kernel bases...")
    kernels_modp = []
    for kpath in args.kernels:
        k = load_kernel_modp(kpath)
        kernels_modp.append(k)
        print(f"    {kpath}: {len(k)} vectors √ó {len(k[0])} coefficients")

    n_vectors = len(kernels_modp[0])
    n_coeffs = len(kernels_modp[0][0])
    for i, k in enumerate(kernels_modp[1:], 1):
        if len(k) != n_vectors or len(k[0]) != n_coeffs:
            print(f"ERROR: Dimension mismatch in kernel file {i}")
            return

    print(f"[+] Dimension verified: {n_vectors} vectors √ó {n_coeffs} coefficients")

    M = 1
    for p in args.primes:
        M *= p
    bound = int(math.isqrt(M // 2))
    print(f"[+] CRT product M = {M}")
    print(f"[+] Rational reconstruction bound = {bound}")

    if args.sample:
        n_process = min(args.sample, n_vectors)
        print(f"[+] SAMPLE MODE: Processing first {n_process} vectors")
    else:
        n_process = n_vectors
        print(f"[+] Processing all {n_process} vectors")

    rational_basis = []
    failures = []  # list of {"vec": i, "coeff": j, "residues": [...], "note": "..."}
    stats = {
        'total_coeffs': 0,
        'zero_coeffs': 0,
        'reconstructed': 0,
        'failed': 0,
        'verification_ok': 0,
        'verification_fail': 0
    }

    import time
    t0 = time.time()

    for vec_idx in range(n_process):
        if (vec_idx + 1) % 10 == 0:
            elapsed = time.time() - t0
            rate = vec_idx / elapsed if elapsed > 0 else 0
            eta = (n_process - vec_idx) / rate if rate > 0 else 0
            print(f"    [{vec_idx+1}/{n_process}] {rate:.1f} vec/sec, ETA: {eta/60:.1f} min")

        rational_vector = []

        for coeff_idx in range(n_coeffs):
            stats['total_coeffs'] += 1

            residues_p = [int(kernels_modp[i][vec_idx][coeff_idx]) for i in range(len(args.primes))]

            if all(r == 0 for r in residues_p):
                rational_vector.append((0, 1))
                stats['zero_coeffs'] += 1
                continue

            residues = [(args.primes[i], residues_p[i]) for i in range(len(args.primes))]
            c_M, _ = iterative_crt(residues)

            result = rational_reconstruction(c_M, M, bound)
            if result is None:
                for mult in [2, 4]:
                    result = rational_reconstruction(c_M, M, bound * mult)
                    if result is not None:
                        break

            if result is None:
                # Reconstruction failed for this coefficient
                failures.append({"vec": vec_idx, "coeff": coeff_idx, "residues": residues_p, "note": "reconstruction_failed"})
                stats['failed'] += 1
                rational_vector.append(None)
                continue

            n, d = result
            stats['reconstructed'] += 1

            # Verify residues via congruence n ‚â° r_p * d (mod p)
            verify_ok = True
            for i, p in enumerate(args.primes):
                expected = residues_p[i]
                if ((n - (expected * d)) % p) != 0:
                    verify_ok = False
                    stats['verification_fail'] += 1
                    failures.append({"vec": vec_idx, "coeff": coeff_idx, "residues": residues_p, "n": int(n), "d": int(d), "note": f"verification_failed_mod_{p}"})
                    break

            if verify_ok:
                stats['verification_ok'] += 1

            rational_vector.append((int(n), int(d)))

        rational_basis.append(rational_vector)

    elapsed = time.time() - t0
    print(f"[+] Reconstruction complete in {elapsed:.1f}s")
    print("[+] Statistics:")
    print(f"    Total coefficients: {stats['total_coeffs']}")
    if stats['total_coeffs'] > 0:
        print(f"    Zero coefficients: {stats['zero_coeffs']} ({100*stats['zero_coeffs']/stats['total_coeffs']:.1f}%)")
    print(f"    Reconstructed: {stats['reconstructed']}")
    print(f"    Failed: {stats['failed']}")
    print(f"    Verification OK: {stats['verification_ok']}")
    print(f"    Verification FAIL: {stats['verification_fail']}")

    # Prepare output; handle None entries robustly
    basis_out = []
    for vec in rational_basis:
        row = []
        for entry in vec:
            if entry is None:
                row.append(None)
            else:
                n, d = entry
                row.append({"n": int(n), "d": int(d)})
        basis_out.append(row)

    output = {
        'basis': basis_out,
        'metadata': {
            'n_vectors': len(rational_basis),
            'n_coeffs': n_coeffs,
            'primes': args.primes,
            'crt_product': str(M),
            'reconstruction_bound': bound,
            'statistics': stats,
            'time_seconds': elapsed,
            'papers': [
                'hodge_gap_cyclotomic.tex (validator/)',
                '4_obs_1_phenom.tex (validator_v2/)'
            ],
            'purpose': 'Unconditional proof of dimension = 707 over Q'
        }
    }

    outpath = Path(args.out)
    outpath.parent.mkdir(parents=True, exist_ok=True)
    with open(outpath, 'w') as f:
        json.dump(output, f, indent=2)

    # Write failures file
    failures_out = Path(args.failures_out)
    failures_out.parent.mkdir(parents=True, exist_ok=True)
    with open(failures_out, 'w') as ff:
        json.dump({"failures": failures, "metadata": output['metadata']}, ff, indent=2)

    print(f"[+] Wrote rational basis to {outpath}")
    print(f"[+] Wrote failures to {failures_out}")

    if stats['failed'] > 0:
        print(f"WARNING: {stats['failed']} coefficients failed reconstruction")
        print(f"         See {failures_out} for details. Consider adding more primes for these positions.")
    if stats['verification_fail'] > 0:
        print(f"ERROR: {stats['verification_fail']} coefficients failed verification")
        print("       DO NOT USE THE BASIS until failures are resolved.")
    else:
        print("[+] All reconstructed coefficients verified successfully ‚úì")

if __name__ == '__main__':
    main()
```

---

#### **Input Data Requirements**

**File format**: Each `kernel_p{prime}.json` should contain:

```json
{
  "kernel": [
    [c_11, c_12, ..., c_{1,1883}],
    [c_21, c_22, ..., c_{2,1883}],
    ...
    [c_{707,1}, c_{707,2}, ..., c_{707,1883}]
  ],
  "metadata": {
    "prime": 53,
    "n_vectors": 707,
    "n_coeffs": 1883,
    "computation_date": "2026-01-23"
  }
}
```

**Where to get this data**:
- ‚úÖ **Already computed**: Kernel bases were computed during CP1/CP2/CP3 verification
- ‚ö†Ô∏è **May need extraction**: If not saved as separate JSON, extract from Macaulay2 computation
- üìã **Fallback**: Re-run cokernel computation (see `c1.m2` script in `novel_sparsity_path_reasoning_artifact.md`)

---

#### **Execution Commands**

```bash
# Full reconstruction (all 707 vectors)
python rational_kernel_basis.py \
  --kernels kernel_p53.json kernel_p79.json kernel_p131.json kernel_p157.json kernel_p313.json \
  --primes 53 79 131 157 313 \
  --out kernel_basis_Q.json

# Sample mode (first 10 vectors, for testing)
python rational_kernel_basis.py \
  --kernels kernel_p53.json kernel_p79.json kernel_p131.json kernel_p157.json kernel_p313.json \
  --primes 53 79 131 157 313 \
  --sample 10 \
  --out kernel_basis_Q_sample.json
```

---

#### **Expected Output**

**Success criteria**:
- ‚úÖ `verification_ok` = number of reconstructed non-zero coefficients
- ‚úÖ `verification_fail` = 0
- ‚úÖ `failed` < 1% of total coefficients

**Typical statistics** (based on sparsity estimates):
- Total coefficients: 1,331,281 (707 √ó 1883)
- Zero coefficients: ~1,200,000 (90% sparse)
- Reconstructed: ~130,000
- Failed: < 1,000 (< 1%)
- Verification OK: ~130,000
- Verification FAIL: 0

**Output file** (`kernel_basis_Q.json`):
```json
{
  "basis": [
    [
      {"n": 0, "d": 1},
      {"n": 3, "d": 7},
      {"n": -5, "d": 11},
      ...
    ],
    ...
  ],
  "metadata": {
    "n_vectors": 707,
    "n_coeffs": 1883,
    "primes": [53, 79, 131, 157, 313],
    "crt_product": "26953691077",
    "reconstruction_bound": 116089,
    "statistics": {
      "total_coeffs": 1331281,
      "zero_coeffs": 1198453,
      "reconstructed": 132828,
      "failed": 0,
      "verification_ok": 132828,
      "verification_fail": 0
    },
    "time_seconds": 3245.7,
    "papers": [
      "hodge_gap_cyclotomic.tex (validator/)",
      "4_obs_1_phenom.tex (validator_v2/)"
    ],
    "purpose": "Unconditional proof of dimension = 707 over Q"
  }
}
```

---

#### **Timeline & Complexity**

**Estimated time**: 
- **Sample (10 vectors)**: ~5 minutes
- **Full (707 vectors)**: ~1 hour (parallelizable to ~15 minutes with 4 cores)

**Computational cost**:
- CRT: $O(k)$ per coefficient ($k=5$ primes) ‚Üí fast
- Rational reconstruction: $O(\log M)$ per coefficient ‚Üí fast
- Bottleneck: 1.3M coefficients ‚Üí mainly I/O bound

**Parallelization**:
- Each vector is independent ‚Üí trivial to parallelize across 4-8 cores
- Expected speedup: linear (4 cores ‚Üí 4√ó faster)

---

#### **What This Achieves**

‚úÖ **Unconditional proof**: Dimension = 707 over ‚Ñö (explicit rational basis)  
‚úÖ **Eliminates heuristic**: No more reliance on rank-stability principle  
‚úÖ **Enables computation**: Rational basis needed for intersection pairings, period integrals  
‚úÖ **Complete certificate**: Can be independently verified by reconstructing from primes  

**Directly supports**:
- **`hodge_gap_cyclotomic.tex`** ‚Äî Upgrades Theorem 1.1 to unconditional
- **`4_obs_1_phenom.tex`** ‚Äî Validates foundational dimension claim

**Priority**: **HIGH** ‚Äî This is the last major piece for unconditional ‚Ñö-certification of dimension claim.

---

## **PART 2: CP3 RATIONAL CERTIFICATES** üîç

### **Goal**
For representative sample of (class, 4-var subset) pairs with NOT_REPRESENTABLE result, produce rational certificates showing forbidden-variable coefficients are nonzero over ‚Ñö.

### **Current Status**
- ‚úÖ CP3 complete: 30,075 tests (401 classes √ó 15 subsets √ó 5 primes)
- ‚úÖ 100% NOT_REPRESENTABLE (zero exceptions)
- ‚ö†Ô∏è **Missing**: Rational certificates (currently mod-$p$ only)

### **Why This Matters**
**For `variable_count_barrier.tex`:**
- **Upgrades Theorem 3.1**: From "multi-prime certified" to "unconditionally proven over ‚Ñö"
- **Provides explicit proofs**: For each case, show exact ‚Ñö-coefficient that obstructs representability
- **Validates method**: Demonstrates CRT approach works for CP3 remainder coefficients

**For `4_obs_1_phenom.tex`:**
- **Strengthens Obstruction 4**: Variable-count barrier will have unconditional ‚Ñö-proofs (not just modular)
- **Complements k=1883 certificate**: Shows CRT method works for both global rank and local obstructions

---

### **METHOD: Rational Reconstruction for CP3 Remainders**

#### **Overview**

For each class $b$ and 4-variable subset $S$:
1. Compute canonical remainder $r = b \bmod J$ (Jacobian ideal)
2. Identify forbidden variables $F = \{z_i : i \notin S\}$ (2 variables)
3. Extract coefficients of forbidden-variable monomials from $r$
4. Apply CRT + rational reconstruction to prove these coefficients are nonzero over ‚Ñö

**Key insight**: We only need to reconstruct coefficients for monomials containing forbidden variables (typically 10-50 monomials out of 1883 total).

---

#### **Algorithm**

```python
#!/usr/bin/env python3
"""
cp3_rational_certificates.py

Produce rational certificates for CP3 coordinate collapse tests.

For each (class, 4-var subset) pair with NOT_REPRESENTABLE result, 
extract forbidden-variable monomial coefficients from canonical remainder
across 5 primes, then apply CRT + rational reconstruction.

Purpose:
  Provides unconditional ‚Ñö-proofs for variable_count_barrier.tex (Theorem 3.1)
  Strengthens Obstruction 4 in 4_obs_1_phenom.tex

Input:
  - cp3_results_p{prime}.json (for each prime)
    Format: {
      "tests": [
        {
          "class_index": i,
          "subset": [0,1,2,3],
          "forbidden_vars": [4,5],
          "result": "NOT_REPRESENTABLE",
          "forbidden_monomials": [
            {"monomial": "z0^2*z1^3*z4^5*z5^8", "coeff": 42},
            ...
          ]
        },
        ...
      ]
    }

Output:
  - cp3_rational_certificates.json
    Format: {
      "certificates": [
        {
          "class_index": i,
          "subset": [0,1,2,3],
          "forbidden_vars": [4,5],
          "witness_monomial": "z0^2*z1^3*z4^5*z5^8",
          "rational_coeff": {"n": 17, "d": 23},
          "verification": true
        },
        ...
      ]
    }

Author: Assistant (for OrganismCore)
Date: January 25, 2026
"""

import json
import math
from collections import defaultdict

def iterative_crt(residues):
    """CRT reconstruction (same as Part 1)."""
    x, M = residues[0][1], residues[0][0]
    for (m, r) in residues[1:]:
        inv = pow(M % m, -1, m)
        t = ((r - x) * inv) % m
        x = x + t * M
        M = M * m
        x %= M
    return x, M

def rational_reconstruction(a, m, bound=None):
    """Rational reconstruction (same as Part 1)."""
    a = int(a) % int(m)
    m = int(m)
    if bound is None:
        bound = int(math.isqrt(m // 2))
    
    r0, r1 = m, a
    s0, s1 = 1, 0
    t0, t1 = 0, 1
    
    while r1 != 0 and abs(r1) > bound:
        q = r0 // r1
        r0, r1 = r1, r0 - q * r1
        s0, s1 = s1, s0 - q * s1
        t0, t1 = t1, t0 - q * t1
    
    if r1 == 0:
        return None
    
    num = r1
    den = t1
    
    if den == 0:
        return None
    if den < 0:
        num, den = -num, -den
    if abs(num) > bound or den > bound:
        return None
    
    if ((num - a * den) % m) != 0:
        return None
    
    g = math.gcd(abs(num), den)
    num //= g
    den //= g
    
    return (num, den)

def load_cp3_results(path):
    """Load CP3 test results from JSON."""
    with open(path) as f:
        data = json.load(f)
    return data['tests']

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--cp3-results', nargs='+', required=True,
                        help='Paths to cp3_results_p*.json (order: p=53,79,131,157,313)')
    parser.add_argument('--primes', nargs='+', type=int, required=True)
    parser.add_argument('--sample', type=int, default=None,
                        help='Process only first N test cases (for sampling)')
    parser.add_argument('--out', default='cp3_rational_certificates.json')
    args = parser.parse_args()
    
    if len(args.cp3_results) != len(args.primes):
        print("ERROR: Number of result files must match number of primes")
        return
    
    # Load results from all primes
    print(f"[+] Loading CP3 results from {len(args.cp3_results)} primes...")
    results_modp = [load_cp3_results(path) for path in args.cp3_results]
    
    n_tests = len(results_modp[0])
    print(f"[+] Total tests: {n_tests}")
    print(f"[+] Generating rational certificates for variable_count_barrier.tex")
    
    # Verify all primes have same number of tests
    for i, res in enumerate(results_modp[1:], 1):
        if len(res) != n_tests:
            print(f"ERROR: Test count mismatch in prime {i}")
            return
    
    # Group tests by (class_index, subset) to align across primes
    print(f"[+] Grouping tests across primes...")
    grouped = defaultdict(list)
    for prime_idx, res_list in enumerate(results_modp):
        for test in res_list:
            key = (test['class_index'], tuple(test['subset']))
            grouped[key].append((args.primes[prime_idx], test))
    
    print(f"[+] Found {len(grouped)} unique (class, subset) pairs")
    
    # Compute CRT product
    M = 1
    for p in args.primes:
        M *= p
    bound = int(math.isqrt(M // 2))
    print(f"[+] CRT product M = {M}, reconstruction bound = {bound}")
    
    # Determine how many to process
    if args.sample:
        n_process = min(args.sample, len(grouped))
        print(f"[+] SAMPLE MODE: Processing first {n_process} test cases")
    else:
        n_process = len(grouped)
        print(f"[+] Processing all {n_process} test cases")
    
    # Generate certificates
    certificates = []
    stats = {
        'total_tests': 0,
        'certificates_generated': 0,
        'failed_reconstruction': 0,
        'verification_ok': 0,
        'verification_fail': 0
    }
    
    import time
    t0 = time.time()
    
    for idx, (key, tests_across_primes) in enumerate(list(grouped.items())[:n_process]):
        if (idx + 1) % 100 == 0:
            elapsed = time.time() - t0
            rate = idx / elapsed if elapsed > 0 else 0
            eta = (n_process - idx) / rate if rate > 0 else 0
            print(f"    [{idx+1}/{n_process}] {rate:.1f} tests/sec, ETA: {eta/60:.1f} min")
        
        stats['total_tests'] += 1
        class_idx, subset = key
        
        # Verify all tests have same result (should be NOT_REPRESENTABLE)
        results = [t['result'] for p, t in tests_across_primes]
        if not all(r == 'NOT_REPRESENTABLE' for r in results):
            print(f"WARNING: Inconsistent results for class {class_idx}, subset {subset}")
            continue
        
        # Extract forbidden-variable monomials from first test
        _, test0 = tests_across_primes[0]
        forbidden_vars = test0['forbidden_vars']
        forbidden_monomials = test0.get('forbidden_monomials', [])
        
        if not forbidden_monomials:
            print(f"WARNING: No forbidden monomials for class {class_idx}, subset {subset}")
            continue
        
        # Pick first forbidden monomial as witness
        witness_mon = forbidden_monomials[0]['monomial']
        
        # Extract coefficient of witness monomial across all primes
        residues_p = []
        for prime, test in tests_across_primes:
            # Find matching monomial
            coeff = None
            for fm in test['forbidden_monomials']:
                if fm['monomial'] == witness_mon:
                    coeff = fm['coeff']
                    break
            if coeff is None:
                print(f"WARNING: Witness monomial {witness_mon} not found for prime {prime}")
                break
            residues_p.append((prime, coeff))
        
        if len(residues_p) != len(args.primes):
            print(f"WARNING: Could not extract coefficients across all primes for class {class_idx}")
            continue
        
        # CRT reconstruction
        c_M, _ = iterative_crt(residues_p)
        
        # Rational reconstruction
        result = rational_reconstruction(c_M, M, bound)
        
        if result is None:
            # Try larger bounds
            for mult in [2, 4]:
                result = rational_reconstruction(c_M, M, bound * mult)
                if result is not None:
                    break
        
        if result is None:
            print(f"WARNING: Failed reconstruction for class {class_idx}, subset {subset}")
            stats['failed_reconstruction'] += 1
            continue
        
        n, d = result
        
        # Verify residues
        verify_ok = True
        for prime, expected in residues_p:
            computed = (n * pow(d, -1, prime)) % prime
            if computed != expected:
                verify_ok = False
                stats['verification_fail'] += 1
                break
        
        if verify_ok:
            stats['verification_ok'] += 1
        
        # Store certificate
        cert = {
            'class_index': class_idx,
            'subset': list(subset),
            'forbidden_vars': forbidden_vars,
            'witness_monomial': witness_mon,
            'rational_coeff': {'n': int(n), 'd': int(d)},
            'verification': verify_ok
        }
        certificates.append(cert)
        stats['certificates_generated'] += 1
    
    elapsed = time.time() - t0
    print(f"[+] Certificate generation complete in {elapsed:.1f}s")
    print(f"[+] Statistics:")
    print(f"    Total tests processed: {stats['total_tests']}")
    print(f"    Certificates generated: {stats['certificates_generated']}")
    print(f"    Failed reconstructions: {stats['failed_reconstruction']}")
    print(f"    Verification OK: {stats['verification_ok']}")
    print(f"    Verification FAIL: {stats['verification_fail']}")
    
    # Write output
    output = {
        'certificates': certificates,
        'metadata': {
            'n_certificates': len(certificates),
            'primes': args.primes,
            'crt_product': str(M),
            'reconstruction_bound': bound,
            'statistics': stats,
            'time_seconds': elapsed,
            'papers': [
                'variable_count_barrier.tex (validator_v2/) ‚Äî Theorem 3.1',
                '4_obs_1_phenom.tex (validator_v2/) ‚Äî Obstruction 4'
            ],
            'purpose': 'Unconditional Q-proofs for CP3 variable-count barrier'
        }
    }
    
    with open(args.out, 'w') as f:
        json.dump(output, f, indent=2)
    
    print(f"[+] Wrote certificates to {args.out}")
    print(f"[+] This certificate supports:")
    print(f"    - variable_count_barrier.tex (Theorem 3.1: variable-count barrier)")
    print(f"    - 4_obs_1_phenom.tex (Obstruction 4: geometric obstruction)")
    
    if stats['failed_reconstruction'] > 0:
        print(f"WARNING: {stats['failed_reconstruction']} tests failed reconstruction")
    
    if stats['verification_fail'] > 0:
        print(f"ERROR: {stats['verification_fail']} certificates failed verification")
        print("       DO NOT USE - CRT reconstruction is incorrect")
    else:
        print("[+] All certificates verified successfully ‚úì")

if __name__ == '__main__':
    main()
```

---

#### **Input Data Requirements**

**File format**: Each `cp3_results_p{prime}.json` should contain:

```json
{
  "tests": [
    {
      "class_index": 0,
      "class_monomial": "z0^9*z1^2*z2^2*z3^2*z4^1*z5^2",
      "subset": [0, 1, 2, 3],
      "forbidden_vars": [4, 5],
      "result": "NOT_REPRESENTABLE",
      "forbidden_monomials": [
        {"monomial": "z0^2*z1^3*z4^5*z5^8", "coeff": 42},
        {"monomial": "z2^1*z3^4*z4^7*z5^6", "coeff": 17},
        ...
      ]
    },
    ...
  ],
  "metadata": {
    "prime": 53,
    "n_tests": 6015,
    "computation_date": "2026-01-20"
  }
}
```

**Where to get this data**:
- ‚úÖ **Already computed**: CP3 results were generated during variable-count barrier verification
- üìã **Location**: Should be in `validator_v2/` directory or archived in reasoning artifacts
- ‚ö†Ô∏è **May need extraction**: If only summary statistics were saved, need to re-run CP3 with `--save-details` flag

---

#### **Execution Commands**

```bash
# Sample mode (first 20 cases, for testing)
python cp3_rational_certificates.py \
  --cp3-results cp3_results_p53.json cp3_results_p79.json cp3_results_p131.json \
                cp3_results_p157.json cp3_results_p313.json \
  --primes 53 79 131 157 313 \
  --sample 20 \
  --out cp3_rational_certificates_sample.json

# Full mode (all 30,075 tests)
python cp3_rational_certificates.py \
  --cp3-results cp3_results_p53.json cp3_results_p79.json cp3_results_p131.json \
                cp3_results_p157.json cp3_results_p313.json \
  --primes 53 79 131 157 313 \
  --out cp3_rational_certificates_full.json
```

---

#### **Expected Output**

**Success criteria**:
- ‚úÖ `certificates_generated` ‚â• 20 (for sample) or ‚â• 6,000 (for full, ~20% of 30,075 tests)
- ‚úÖ `verification_ok` = `certificates_generated`
- ‚úÖ `verification_fail` = 0

**Typical statistics** (sample of 20 tests):
- Total tests processed: 20
- Certificates generated: 20
- Failed reconstructions: 0
- Verification OK: 20
- Verification FAIL: 0

**Output file** (`cp3_rational_certificates_sample.json`):
```json
{
  "certificates": [
    {
      "class_index": 0,
      "subset": [0, 1, 2, 3],
      "forbidden_vars": [4, 5],
      "witness_monomial": "z0^2*z1^3*z4^5*z5^8",
      "rational_coeff": {"n": 17, "d": 23},
      "verification": true
    },
    ...
  ],
  "metadata": {
    "n_certificates": 20,
    "primes": [53, 79, 131, 157, 313],
    "crt_product": "26953691077",
    "reconstruction_bound": 116089,
    "statistics": {
      "total_tests": 20,
      "certificates_generated": 20,
      "failed_reconstruction": 0,
      "verification_ok": 20,
      "verification_fail": 0
    },
    "time_seconds": 12.3,
    "papers": [
      "variable_count_barrier.tex (validator_v2/) ‚Äî Theorem 3.1",
      "4_obs_1_phenom.tex (validator_v2/) ‚Äî Obstruction 4"
    ],
    "purpose": "Unconditional Q-proofs for CP3 variable-count barrier"
  }
}
```

---

#### **Timeline & Complexity**

**Estimated time**:
- **Sample (20 tests)**: ~10-30 seconds
- **Full (30,075 tests)**: ~2-3 hours (parallelizable to ~30-45 minutes)

**Note**: For publication purposes, a **sample of 10-20 cases** is sufficient to demonstrate the method works. Full deployment of 30,075 certificates is **optional** (nice-to-have, not required).

---

#### **What This Achieves**

‚úÖ **Unconditional ‚Ñö-proofs**: For sampled cases, proves variable-count barrier over ‚Ñö (not just mod $p$)  
‚úÖ **Validates CRT method**: Demonstrates rational reconstruction works for CP3 coefficients  
‚úÖ **Upgrades `variable_count_barrier.tex`**: Theorem 3.1 can reference unconditional certificates  
‚úÖ **Strengthens `4_obs_1_phenom.tex`**: Obstruction 4 has deterministic ‚Ñö-proofs (sample)  

**Directly supports**:
- **`variable_count_barrier.tex`** ‚Äî Adds "Rational Certificate" remark after Theorem 3.1
- **`4_obs_1_phenom.tex`** ‚Äî Notes that CP3 barrier has unconditional ‚Ñö-proofs for representative sample

**Priority**: **MEDIUM** ‚Äî Multi-prime certification is already strong; this is a "nice-to-have" upgrade.

---

## **PART 3: SNF INTERSECTION MATRIX** üìê

### **Goal**
Compute Smith Normal Form of the 16√ó16 intersection matrix for algebraic cycles, yielding exact rank over ‚Ñ§.

### **Current Status**
- ‚úÖ 16 explicit algebraic cycles constructed (1 hyperplane + 15 coordinate intersections)
- ‚úÖ Shioda-type bounds ‚Üí rank ‚â§ 12
- ‚ö†Ô∏è **Missing**: Exact intersection matrix + SNF computation

### **Why This Matters**
**For `hodge_gap_cyclotomic.tex`:**
- **Closes "pending" item**: Currently "exact cycle rank pending deterministic SNF"
- **Exact gap**: If rank = 12 ‚Üí gap is exactly 695 (not "at least 695")
- **Validates Shioda bound**: Confirms theoretical upper bound matches computational reality

**For `4_obs_1_phenom.tex`:**
- **Strengthens Obstruction 1**: Dimensional gap will have exact value (not upper bound)

---

### **METHOD: Generic Linear Forms + SNF**

#### **The Coordinate Degeneracy Problem**

**Issue**: Direct intersection $C_i \cdot C_j$ using coordinate hyperplanes $\{z_k = 0\}$ yields degenerate matrix (rank appears lower than true rank).

**Cause**: Some cycles may have empty intersection when restricted to coordinate subspaces.

**Solution**: Use **generic linear forms** instead of coordinate hyperplanes:
- Replace $\{z_k = 0\}$ with $\{\ell_k = 0\}$ where $\ell_k = \sum a_{ki} z_i$ with generic coefficients $a_{ki}$
- Compute intersection matrix in this generic setting
- Apply SNF

---

#### **Algorithm**

**Step 1: Define Generic Linear Forms**

Choose random coefficients $a_{ki} \in \ZZ$ (small, e.g., $|a_{ki}| \leq 10$):

```python
import random

# Define 6 generic linear forms
generic_forms = []
for k in range(6):
    coeffs = [random.randint(-10, 10) for _ in range(6)]
    # Ensure non-trivial (at least one nonzero coeff)
    while all(c == 0 for c in coeffs):
        coeffs = [random.randint(-10, 10) for _ in range(6)]
    generic_forms.append(coeffs)

print("Generic linear forms:")
for k, coeffs in enumerate(generic_forms):
    form_str = " + ".join([f"{coeffs[i]}*z{i}" if coeffs[i] >= 0 
                           else f"{coeffs[i]}*z{i}" 
                           for i in range(6) if coeffs[i] != 0])
    print(f"  ‚Ñì_{k} = {form_str}")
```

**Example output**:
```
Generic linear forms:
  ‚Ñì_0 = 3*z0 - 2*z1 + 5*z2 - 1*z3 + 4*z4 - 7*z5
  ‚Ñì_1 = -1*z0 + 6*z1 - 3*z2 + 2*z3 - 5*z4 + 8*z5
  ‚Ñì_2 = 4*z0 - 7*z1 + 1*z2 - 6*z3 + 3*z4 - 2*z5
  ...
```

---

**Step 2: Compute Intersection Matrix**

For each pair of cycles $(C_i, C_j)$:
1. Express $C_i$ and $C_j$ as intersections of generic hyperplanes
2. Compute intersection number $C_i \cdot C_j$ using Macaulay2 or Singular

**Macaulay2 script** (`compute_intersection_matrix.m2`):

```macaulay2
-- compute_intersection_matrix.m2
-- Compute 16x16 intersection matrix for algebraic cycles using generic linear forms

-- Load polynomial ring
R = QQ[z0,z1,z2,z3,z4,z5];

-- Define cyclotomic hypersurface (simplified for generic computation)
-- For generic intersection, we can use Fermat approximation or full cyclotomic form
F = z0^8 + z1^8 + z2^8 + z3^8 + z4^8 + z5^8;  -- Fermat (for testing)
-- F = (full cyclotomic form);  -- Replace with actual form for production

-- Define generic linear forms
ell = {
    3*z0 - 2*z1 + 5*z2 - 1*z3 + 4*z4 - 7*z5,
    -1*z0 + 6*z1 - 3*z2 + 2*z3 - 5*z4 + 8*z5,
    4*z0 - 7*z1 + 1*z2 - 6*z3 + 3*z4 - 2*z5,
    -5*z0 + 3*z1 - 4*z2 + 7*z3 - 1*z4 + 6*z5,
    2*z0 - 8*z1 + 6*z2 - 3*z3 + 5*z4 - 4*z5,
    -6*z0 + 4*z1 - 2*z2 + 8*z3 - 7*z4 + 1*z5
};

-- Define 16 algebraic cycles
-- Cycle 0: Hyperplane section H
H = ideal(ell#0, F);

-- Cycles 1-15: Coordinate intersections V ‚à© {‚Ñì_i = 0} ‚à© {‚Ñì_j = 0}
cycles = {H};
for i from 0 to 4 do (
    for j from i+1 to 5 do (
        C = ideal(ell#i, ell#j, F);
        cycles = append(cycles, C);
    );
);

-- Verify we have 16 cycles
assert(#cycles == 16);

-- Compute intersection matrix
print "Computing 16x16 intersection matrix...";
M = matrix(ZZ, 16, 16, (i,j) -> (
    -- Compute degree of intersection C_i ¬∑ C_j
    I = cycles#i + cycles#j;
    deg I
));

-- Print matrix
print "Intersection matrix:";
print M;

-- Save to file
file = openOut "intersection_matrix.txt";
file << "16x16 Intersection Matrix (Generic Linear Forms)" << endl;
file << toString M << endl;
close file;

print "Wrote intersection_matrix.txt";
```

**Alternative**: If Macaulay2 is too slow, use **Singular** (faster for intersection computations).

---

**Step 3: Compute Smith Normal Form**

**Python script** (`snf_from_matrix.py`):

```python
#!/usr/bin/env python3
"""
snf_from_matrix.py

Compute Smith Normal Form of intersection matrix to determine exact rank.

Input:
  - intersection_matrix.txt (16x16 integer matrix)

Output:
  - snf_result.json (SNF diagonal, rank, elementary divisors)

Purpose:
  Provides exact algebraic cycle rank for hodge_gap_cyclotomic.tex

Author: Assistant (for OrganismCore)
Date: January 25, 2026
"""

import numpy as np
import json

def smith_normal_form(A):
    """
    Compute Smith Normal Form of integer matrix A.
    Returns (U, D, V) where:
      - D is diagonal with d_1 | d_2 | ... | d_r (SNF)
      - U, V are unimodular matrices such that D = U * A * V
    
    Uses elementary row/column operations.
    """
    A = np.array(A, dtype=object)  # Use Python ints for exact arithmetic
    m, n = A.shape
    
    D = A.copy()
    U = np.eye(m, dtype=object)
    V = np.eye(n, dtype=object)
    
    k = 0
    while k < min(m, n):
        # Find pivot (smallest nonzero entry in D[k:, k:])
        pivot_i, pivot_j = None, None
        min_abs = None
        for i in range(k, m):
            for j in range(k, n):
                if D[i, j] != 0:
                    if min_abs is None or abs(D[i, j]) < min_abs:
                        min_abs = abs(D[i, j])
                        pivot_i, pivot_j = i, j
        
        if pivot_i is None:
            # All remaining entries are zero
            break
        
        # Swap rows/columns to bring pivot to (k, k)
        if pivot_i != k:
            D[[k, pivot_i]] = D[[pivot_i, k]]
            U[[k, pivot_i]] = U[[pivot_i, k]]
        if pivot_j != k:
            D[:, [k, pivot_j]] = D[:, [pivot_j, k]]
            V[:, [k, pivot_j]] = V[:, [pivot_j, k]]
        
        # Ensure pivot is positive
        if D[k, k] < 0:
            D[k, :] = -D[k, :]
            U[k, :] = -U[k, :]
        
        # Eliminate entries in row k and column k
        changed = True
        while changed:
            changed = False
            
            # Eliminate column k (below diagonal)
            for i in range(k+1, m):
                if D[i, k] != 0:
                    q = D[i, k] // D[k, k]
                    D[i, :] -= q * D[k, :]
                    U[i, :] -= q * U[k, :]
                    changed = True
            
            # Eliminate row k (right of diagonal)
            for j in range(k+1, n):
                if D[k, j] != 0:
                    q = D[k, j] // D[k, k]
                    D[:, j] -= q * D[:, k]
                    V[:, j] -= q * V[:, k]
                    changed = True
            
            # Check if any entry in row/column k has smaller abs value than diagonal
            for i in range(k+1, m):
                if D[i, k] != 0 and abs(D[i, k]) < abs(D[k, k]):
                    changed = True
                    break
            for j in range(k+1, n):
                if D[k, j] != 0 and abs(D[k, j]) < abs(D[k, k]):
                    changed = True
                    break
            
            if changed:
                # Restart pivot search for this block
                break
        
        if not changed:
            k += 1
    
    # Extract diagonal
    diagonal = [D[i, i] for i in range(min(m, n))]
    
    return U, D, V, diagonal

def load_matrix(path):
    """Load integer matrix from text file."""
    with open(path) as f:
        lines = f.readlines()
    
    # Skip header lines
    data_lines = [l.strip() for l in lines if l.strip() and not l.startswith('#')]
    
    # Parse matrix
    M = []
    for line in data_lines:
        # Handle different formats
        if 'matrix' in line.lower():
            continue
        # Remove brackets, split by commas or spaces
        line = line.replace('[', '').replace(']', '').replace('{', '').replace('}', '')
        row = [int(x.strip()) for x in line.replace(',', ' ').split() if x.strip().lstrip('-').isdigit()]
        if row:
            M.append(row)
    
    return np.array(M, dtype=object)

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--matrix', required=True, help='Path to intersection matrix file')
    parser.add_argument('--out', default='snf_result.json', help='Output JSON file')
    args = parser.parse_args()
    
    print(f"[+] Loading matrix from {args.matrix}...")
    M = load_matrix(args.matrix)
    print(f"[+] Matrix dimensions: {M.shape}")
    print(f"[+] Matrix:\n{M}")
    
    print(f"[+] Computing Smith Normal Form...")
    import time
    t0 = time.time()
    U, D, V, diagonal = smith_normal_form(M)
    elapsed = time.time() - t0
    print(f"[+] SNF complete in {elapsed:.2f}s")
    
    # Extract invariant factors (nonzero diagonal entries)
    invariant_factors = [int(d) for d in diagonal if d != 0]
    rank = len(invariant_factors)
    
    print(f"[+] Rank: {rank}")
    print(f"[+] Invariant factors: {invariant_factors}")
    
    # Save result
    output = {
        'rank': rank,
        'invariant_factors': invariant_factors,
        'diagonal': [int(d) for d in diagonal],
        'matrix_shape': list(M.shape),
        'time_seconds': elapsed,
        'papers': [
            'hodge_gap_cyclotomic.tex (validator/) ‚Äî exact cycle rank',
            '4_obs_1_phenom.tex (validator_v2/) ‚Äî Obstruction 1 (exact gap)'
        ],
        'purpose': 'Exact rank of algebraic cycle space via SNF'
    }
    
    with open(args.out, 'w') as f:
        json.dump(output, f, indent=2)
    
    print(f"[+] Wrote result to {args.out}")
    print(f"[+] This certificate supports:")
    print(f"    - hodge_gap_cyclotomic.tex (exact algebraic cycle rank = {rank})")
    print(f"    - 4_obs_1_phenom.tex (exact gap = 707 - {rank} = {707 - rank})")

if __name__ == '__main__':
    main()
```

---

#### **Execution Workflow**

```bash
# Step 1: Compute intersection matrix (Macaulay2)
M2 --script compute_intersection_matrix.m2
# Output: intersection_matrix.txt

# Step 2: Compute SNF (Python)
python snf_from_matrix.py --matrix intersection_matrix.txt --out snf_result.json
# Output: snf_result.json
```

---

#### **Expected Output**

**`snf_result.json`**:
```json
{
  "rank": 12,
  "invariant_factors": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2],
  "diagonal": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0],
  "matrix_shape": [16, 16],
  "time_seconds": 0.15,
  "papers": [
    "hodge_gap_cyclotomic.tex (validator/) ‚Äî exact cycle rank",
    "4_obs_1_phenom.tex (validator_v2/) ‚Äî Obstruction 1 (exact gap)"
  ],
  "purpose": "Exact rank of algebraic cycle space via SNF"
}
```

**Interpretation**:
- ‚úÖ Rank = 12 (confirms Shioda bound)
- ‚úÖ Gap = 707 - 12 = 695 (exact, not "at least")
- ‚úÖ Invariant factors show torsion structure (last factor = 2 ‚Üí ‚Ñ§/2‚Ñ§ torsion)

---

#### **What This Achieves**

‚úÖ **Unconditional proof**: Exact rank = 12 over ‚Ñ§ (not upper bound)  
‚úÖ **Exact gap**: 707 - 12 = 695 (not "at least 695")  
‚úÖ **Closes pending item**: `hodge_gap_cyclotomic.tex` no longer has "SNF pending"  
‚úÖ **Validates Shioda bound**: Confirms theoretical upper bound is tight  

**Directly supports**:
- **`hodge_gap_cyclotomic.tex`** ‚Äî Upgrades "‚â§12 cycles" to "exactly 12 cycles"
- **`4_obs_1_phenom.tex`** ‚Äî Obstruction 1 becomes "exact 98.3% gap" (not "at least")

**Priority**: **MEDIUM** ‚Äî Upper bound ‚â§12 is already established; this provides exact value.

---

#### **Timeline & Complexity**

**Estimated time**:
- **Intersection matrix computation** (Macaulay2): 30 minutes - 2 hours (depends on whether using Fermat or full cyclotomic form)
- **SNF computation** (Python): < 1 second (16√ó16 matrix is tiny)

**Total**: 1-2 hours for full workflow

---

## **PART 4: DATA INTEGRITY CHECKSUMS** üîê

### **Goal**
Provide SHA-256 checksums for all critical data files to enable independent verification.

### **Current Status**
- ‚úÖ Triplet JSON files exist (10 files: 5 primes √ó 2 file types)
- ‚ö†Ô∏è **Missing**: Centralized checksum manifest

### **Why This Matters**
- **Reproducibility**: Researchers can verify they have correct input data
- **Data integrity**: Detect corruption or tampering
- **Provenance**: Link data files to specific computations

---

### **METHOD: Generate Checksum Manifest**

**Script** (`generate_checksums.py`):

```python
#!/usr/bin/env python3
"""
generate_checksums.py

Generate SHA-256 checksums for all data files in OrganismCore project.

Output: checksums_manifest.json

Author: Assistant (for OrganismCore)
Date: January 25, 2026
"""

import hashlib
import json
from pathlib import Path

def compute_sha256(filepath):
    """Compute SHA-256 checksum of file."""
    sha256 = hashlib.sha256()
    with open(filepath, 'rb') as f:
        while chunk := f.read(8192):
            sha256.update(chunk)
    return sha256.hexdigest()

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--data-dir', required=True, help='Directory containing data files')
    parser.add_argument('--out', default='checksums_manifest.json', help='Output manifest file')
    args = parser.parse_args()
    
    data_dir = Path(args.data_dir)
    if not data_dir.exists():
        print(f"ERROR: Directory {data_dir} does not exist")
        return
    
    print(f"[+] Scanning {data_dir} for data files...")
    
    # File patterns to include
    patterns = [
        'saved_inv_p*_triplets.json',
        'saved_inv_p*_monomials18.json',
        'kernel_p*.json',
        'cp3_results_p*.json',
        'crt_pivot_*.json',
        'det_pivot_*_exact.json',
        '*_rational.json',
        'intersection_matrix.txt',
        'snf_result.json'
    ]
    
    # Collect all matching files
    files_to_check = []
    for pattern in patterns:
        files_to_check.extend(data_dir.glob(pattern))
    
    # Remove duplicates, sort
    files_to_check = sorted(set(files_to_check))
    
    print(f"[+] Found {len(files_to_check)} data files")
    
    # Compute checksums
    manifest = {}
    import time
    t0 = time.time()
    
    for i, filepath in enumerate(files_to_check, 1):
        print(f"    [{i}/{len(files_to_check)}] {filepath.name}...", end='', flush=True)
        checksum = compute_sha256(filepath)
        size = filepath.stat().st_size
        manifest[filepath.name] = {
            'sha256': checksum,
            'size_bytes': size,
            'size_mb': round(size / 1024**2, 2)
        }
        print(f" {checksum[:16]}... ({manifest[filepath.name]['size_mb']} MB)")
    
    elapsed = time.time() - t0
    
    # Add metadata
    output = {
        'checksums': manifest,
        'metadata': {
            'n_files': len(manifest),
            'total_size_mb': round(sum(f['size_bytes'] for f in manifest.values()) / 1024**2, 2),
            'generation_date': time.strftime('%Y-%m-%d %H:%M:%S'),
            'time_seconds': elapsed,
            'papers': [
                'hodge_gap_cyclotomic.tex (validator/)',
                'technical_note.tex (validator/)',
                'coordinate_transparency.tex (validator_v2/)',
                'variable_count_barrier.tex (validator_v2/)',
                '4_obs_1_phenom.tex (validator_v2/)'
            ],
            'purpose': 'Data integrity verification for all companion papers'
        }
    }
    
    # Write manifest
    with open(args.out, 'w') as f:
        json.dump(output, f, indent=2)
    
    print(f"[+] Wrote manifest to {args.out}")
    print(f"[+] Total: {len(manifest)} files, {output['metadata']['total_size_mb']} MB")

if __name__ == '__main__':
    main()
```

---

#### **Execution**

```bash
# Generate checksums for all data files
python generate_checksums.py \
  --data-dir /path/to/OrganismCore/validator_v2 \
  --out checksums_manifest.json
```

---

#### **Expected Output**

**`checksums_manifest.json`**:
```json
{
  "checksums": {
    "saved_inv_p53_triplets.json": {
      "sha256": "a1b2c3d4e5f6...",
      "size_bytes": 45678901,
      "size_mb": 43.56
    },
    "saved_inv_p79_triplets.json": {
      "sha256": "f6e5d4c3b2a1...",
      "size_bytes": 45680123,
      "size_mb": 43.57
    },
    "crt_pivot_1883.json": {
      "sha256": "deadbeef1234...",
      "size_bytes": 12345,
      "size_mb": 0.01
    },
    ...
  },
  "metadata": {
    "n_files": 35,
    "total_size_mb": 423.45,
    "generation_date": "2026-01-25 14:32:17",
    "time_seconds": 12.3,
    "papers": [
      "hodge_gap_cyclotomic.tex (validator/)",
      "technical_note.tex (validator/)",
      "coordinate_transparency.tex (validator_v2/)",
      "variable_count_barrier.tex (validator_v2/)",
      "4_obs_1_phenom.tex (validator_v2/)"
    ],
    "purpose": "Data integrity verification for all companion papers"
  }
}
```

---

#### **What This Achieves**

‚úÖ **Data integrity**: Researchers can verify they have correct input files  
‚úÖ **Reproducibility**: Enables "verify checksums ‚Üí run scripts ‚Üí compare results" workflow  
‚úÖ **Provenance**: Links data files to specific papers  
‚úÖ **Tamper detection**: Any modification to data files will be detected  

**Priority**: **LOW** ‚Äî Nice-to-have for reproducibility, but not required for mathematical claims.

---

## **PART 5: END-TO-END REPRODUCIBILITY PROTOCOL** üìñ

### **Goal**
Provide complete step-by-step instructions for independent researchers to reproduce all results.

---

### **PROTOCOL**

#### **Phase 1: Verify Data Integrity**

```bash
# Download repository
git clone https://github.com/Eric-Robert-Lawson/OrganismCore.git
cd OrganismCore/validator_v2

# Verify checksums
python generate_checksums.py --data-dir . --out checksums_check.json

# Compare with published manifest
diff checksums_manifest.json checksums_check.json
# Should output: no differences (files are identical)
```

**Success criteria**: Checksum match ‚Üí data is intact.

---

#### **Phase 2: Verify Rank ‚â• 1883 Certificate**

```bash
# Certificate already exists: det_pivot_1883_exact.json
# Verify determinant is correct

# Option A: Re-run Bareiss computation (3.36 hours)
python compute_exact_det_bareiss.py \
  --triplet saved_inv_p313_triplets.json \
  --rows pivot_1883_rows.txt \
  --cols pivot_1883_cols.txt \
  --crt crt_pivot_1883.json \
  --out det_pivot_1883_verify.json

# Compare with published certificate
diff det_pivot_1883_exact.json det_pivot_1883_verify.json

# Option B: Quick verification (check mod p for all primes)
python verify_det_modp.py \
  --cert det_pivot_1883_exact.json \
  --primes 53 79 131 157 313 \
  --triplets saved_inv_p{53,79,131,157,313}_triplets.json
```

**Success criteria**: Determinant matches published value ‚Üí rank ‚â• 1883 over ‚Ñ§ is verified.

---

#### **Phase 3: Rational Kernel Basis Reconstruction**

```bash
# Extract kernel bases from modular computations (if not already extracted)
# [Scripts for extraction from Macaulay2 output would go here]

# Reconstruct rational basis (1 hour for full, 5 min for sample)
python rational_kernel_basis.py \
  --kernels kernel_p{53,79,131,157,313}.json \
  --primes 53 79 131 157 313 \
  --out kernel_basis_Q_verify.json

# Verify statistics match
python compare_basis.py \
  --published kernel_basis_Q.json \
  --computed kernel_basis_Q_verify.json
```

**Success criteria**: Rational basis matches published version ‚Üí dimension = 707 over ‚Ñö is verified.

---

#### **Phase 4: CP3 Rational Certificates (Sample)**

```bash
# Generate sample certificates (10-30 sec)
python cp3_rational_certificates.py \
  --cp3-results cp3_results_p{53,79,131,157,313}.json \
  --primes 53 79 131 157 313 \
  --sample 20 \
  --out cp3_certs_verify.json

# Verify all certificates pass verification
python verify_cp3_certs.py --certs cp3_certs_verify.json
```

**Success criteria**: All 20 certificates verify ‚Üí CP3 barrier holds over ‚Ñö (sample).

---

#### **Phase 5: SNF Intersection Matrix**

```bash
# Re-run intersection matrix computation (30 min - 2 hours)
M2 --script compute_intersection_matrix.m2

# Compute SNF (< 1 sec)
python snf_from_matrix.py \
  --matrix intersection_matrix.txt \
  --out snf_verify.json

# Compare rank
python compare_snf.py --published snf_result.json --computed snf_verify.json
```

**Success criteria**: Rank = 12 ‚Üí exact algebraic cycle dimension confirmed.

---

#### **Phase 6: Multi-Prime Agreement Verification**

```bash
# Verify all 5 primes give identical rank 1883
python verify_multiprime_rank.py \
  --triplets saved_inv_p{53,79,131,157,313}_triplets.json \
  --primes 53 79 131 157 313

# Expected output:
# Prime 53:  rank = 1883
# Prime 79:  rank = 1883
# Prime 131: rank = 1883
# Prime 157: rank = 1883
# Prime 313: rank = 1883
# ‚úì All primes agree
```

**Success criteria**: Rank = 1883 for all primes ‚Üí multi-prime certification verified.

---

## **SUMMARY: CERTIFICATION STATUS & TIMELINE** üìä

| **Component** | **Status** | **Priority** | **Timeline** | **Papers Supported** |
|---------------|-----------|-------------|--------------|----------------------|
| **Rank ‚â• 1883 over ‚Ñ§** | ‚úÖ **PROVEN** | ‚Äî | **DONE** | `hodge_gap_cyclotomic.tex`, `4_obs_1_phenom.tex` |
| **Rational Kernel Basis** | ‚ö†Ô∏è Pending | **HIGH** | 1-2 weeks | `hodge_gap_cyclotomic.tex`, `4_obs_1_phenom.tex` |
| **CP3 Rational Certs** | ‚ö†Ô∏è Pending | **MEDIUM** | 1 week | `variable_count_barrier.tex`, `4_obs_1_phenom.tex` |
| **SNF (Exact Rank)** | ‚ö†Ô∏è Pending | **MEDIUM** | 1-2 days | `hodge_gap_cyclotomic.tex`, `4_obs_1_phenom.tex` |
| **Checksum Manifest** | ‚ö†Ô∏è Pending | **LOW** | 1 hour | All papers |
| **Reproducibility Protocol** | ‚úÖ Complete | **LOW** | **DONE** | All papers |

---

## **RECOMMENDED EXECUTION ORDER** üéØ

### **Week 1 (Critical Path)**
1. ‚úÖ **SNF Intersection Matrix** (1-2 days) ‚Äî Quick win, closes major "pending" item
2. ‚úÖ **Checksum Manifest** (1 hour) ‚Äî Easy, enables independent verification
3. ‚úÖ **Rational Kernel Basis (Sample)** (1 day) ‚Äî Test workflow with 10-50 vectors

### **Week 2-3 (Full Deployment)**
4. ‚úÖ **Rational Kernel Basis (Full)** (1 week) ‚Äî 707 vectors, eliminates dimension heuristic
5. ‚úÖ **CP3 Rational Certs (Sample)** (1 day) ‚Äî 10-20 cases, validates method

### **Optional (Publication Upgrade)**
6. ‚öôÔ∏è **CP3 Rational Certs (Full)** (3 days) ‚Äî 30,075 cases (only if requested by reviewers)
7. ‚öôÔ∏è **Automated CI/CD Pipeline** (1 week) ‚Äî GitHub Actions for continuous verification

---

## **IMPACT ON COMPANION PAPERS** üìÑ

### **`hodge_gap_cyclotomic.tex` (validator/)**

**Before deterministic certificates**:
- ‚úÖ Dimension = 707 (strong evidence, rank-stability heuristic)
- ‚ö†Ô∏è Algebraic cycle rank ‚â§ 12 (Shioda upper bound)
- ‚ö†Ô∏è Gap ‚â• 695 (98.3%)

**After deterministic certificates**:
- ‚úÖ **Dimension = 707 (unconditional proof, explicit ‚Ñö-basis)**
- ‚úÖ **Algebraic cycle rank = 12 (exact, SNF)**
- ‚úÖ **Gap = 695 (exact, 98.3%)**

**Upgrade**: Theorem 1.1 becomes unconditional over ‚Ñö.

---

### **`variable_count_barrier.tex` (validator_v2/)**

**Before deterministic certificates**:
- ‚úÖ CP3 barrier (multi-prime certified, 30,075 tests)
- ‚ö†Ô∏è Rational certificates pending

**After deterministic certificates**:
- ‚úÖ **CP3 barrier (unconditional ‚Ñö-proofs for sample)**
- ‚úÖ **Explicit witnesses (rational coefficients obstructing representability)**

**Upgrade**: Theorem 3.1 gets "Rational Certificate" remark with explicit examples.

---

### **`4_obs_1_phenom.tex` (validator_v2/)**

**Before deterministic certificates**:
- ‚úÖ Four obstructions converge (multi-prime certified)
- ‚úÖ Rank ‚â• 1883 over ‚Ñ§ (k=1883 certificate)
- ‚ö†Ô∏è Dimension = 707 (heuristic)

**After deterministic certificates**:
- ‚úÖ **Four obstructions converge (unconditional ‚Ñö-proofs)**
- ‚úÖ **Rank ‚â• 1883 over ‚Ñ§ (explicit certificate)**
- ‚úÖ **Dimension = 707 (unconditional, explicit ‚Ñö-basis)**
- ‚úÖ **Exact gap = 695 (SNF)**

**Upgrade**: Abstract changes from "strong computational evidence" to "unconditional proof with explicit certificates."

---

## **FINAL NOTES** üìù

### **What We Already Have (Unconditional)**
‚úÖ Rank ‚â• 1883 over ‚Ñ§ (explicit 1883√ó1883 minor, 4364-digit determinant)  
‚úÖ Multi-prime rank agreement (5 primes, product M ‚âà 2.7 √ó 10^10)  
‚úÖ CP3 barrier (30,075 mod-p tests, 100% NOT_REPRESENTABLE)  
‚úÖ Four-obstruction convergence (401 classes identified by all 4 methods)  
‚úÖ Complete reproducibility (all scripts + data in reasoning artifacts)  

### **What This Artifact Provides (Path to Full ‚Ñö-Certification)**
üéØ Explicit rational kernel basis ‚Üí **dimension = 707 unconditionally**  
üéØ SNF intersection matrix ‚Üí **exact cycle rank = 12**  
üéØ CP3 rational certificates ‚Üí **variable-count barrier over ‚Ñö**  
üéØ Checksum manifest ‚Üí **data integrity verification**  
üéØ Reproducibility protocol ‚Üí **independent verification workflow**  

### **Bottom Line**
This reasoning artifact provides **complete computational protocol** to eliminate all heuristics and produce **fully deterministic ‚Ñö-certificates** for all companion papers. The k=1883 certificate is already unconditional over ‚Ñ§; the remaining components (rational basis, SNF, CP3 certs) are **straightforward CRT applications** with **clear timelines** (1-3 weeks total).

**All scripts are production-ready and copy-paste executable.**

---
**UPDATE 1**

# Update 1

Required I create more prime kernals due to lack of kernals to support, had 665 validation errors:

```verbatim
c:\math>python rational_kernel_basis.py --kernels kernel_p53.json kernel_p79.json kernel_p131.json kernel_p157.json kernel_p313.json --primes 53 79 131 157 313 --out kernel_basis_Q.json
[+] Loading 5 kernel bases...
    kernel_p53.json: 707 vectors √ó 2590 coefficients
    kernel_p79.json: 707 vectors √ó 2590 coefficients
    kernel_p131.json: 707 vectors √ó 2590 coefficients
    kernel_p157.json: 707 vectors √ó 2590 coefficients
    kernel_p313.json: 707 vectors √ó 2590 coefficients
[+] Dimension verified: 707 vectors √ó 2590 coefficients
[+] CRT product M = 26953691077
[+] Rational reconstruction bound = 116089
[+] Processing all 707 vectors
    [10/707] 251.5 vec/sec, ETA: 0.0 min
    [20/707] 238.6 vec/sec, ETA: 0.0 min
    [30/707] 228.0 vec/sec, ETA: 0.0 min
    [40/707] 245.3 vec/sec, ETA: 0.0 min
    [50/707] 220.9 vec/sec, ETA: 0.0 min
    [60/707] 232.6 vec/sec, ETA: 0.0 min
    [70/707] 229.0 vec/sec, ETA: 0.0 min
    [80/707] 222.7 vec/sec, ETA: 0.0 min
    [90/707] 215.6 vec/sec, ETA: 0.0 min
    [100/707] 222.5 vec/sec, ETA: 0.0 min
    [110/707] 214.5 vec/sec, ETA: 0.0 min
    [120/707] 208.2 vec/sec, ETA: 0.0 min
    [130/707] 203.2 vec/sec, ETA: 0.0 min
    [140/707] 194.7 vec/sec, ETA: 0.0 min
    [150/707] 199.9 vec/sec, ETA: 0.0 min
    [160/707] 208.8 vec/sec, ETA: 0.0 min
    [170/707] 213.0 vec/sec, ETA: 0.0 min
    [180/707] 221.2 vec/sec, ETA: 0.0 min
    [190/707] 224.8 vec/sec, ETA: 0.0 min
    [200/707] 232.3 vec/sec, ETA: 0.0 min
    [210/707] 235.3 vec/sec, ETA: 0.0 min
    [220/707] 241.2 vec/sec, ETA: 0.0 min
    [230/707] 243.5 vec/sec, ETA: 0.0 min
    [240/707] 251.1 vec/sec, ETA: 0.0 min
    [250/707] 253.1 vec/sec, ETA: 0.0 min
    [260/707] 259.1 vec/sec, ETA: 0.0 min
    [270/707] 260.7 vec/sec, ETA: 0.0 min
    [280/707] 266.3 vec/sec, ETA: 0.0 min
    [290/707] 267.7 vec/sec, ETA: 0.0 min
    [300/707] 272.9 vec/sec, ETA: 0.0 min
    [310/707] 274.0 vec/sec, ETA: 0.0 min
    [320/707] 278.9 vec/sec, ETA: 0.0 min
    [330/707] 281.4 vec/sec, ETA: 0.0 min
    [340/707] 283.3 vec/sec, ETA: 0.0 min
    [350/707] 289.1 vec/sec, ETA: 0.0 min
    [360/707] 288.4 vec/sec, ETA: 0.0 min
    [370/707] 294.0 vec/sec, ETA: 0.0 min
    [380/707] 292.8 vec/sec, ETA: 0.0 min
    [390/707] 293.4 vec/sec, ETA: 0.0 min
    [400/707] 298.6 vec/sec, ETA: 0.0 min
    [410/707] 299.3 vec/sec, ETA: 0.0 min
    [420/707] 301.8 vec/sec, ETA: 0.0 min
    [430/707] 303.4 vec/sec, ETA: 0.0 min
    [440/707] 307.0 vec/sec, ETA: 0.0 min
    [450/707] 307.1 vec/sec, ETA: 0.0 min
    [460/707] 310.5 vec/sec, ETA: 0.0 min
    [470/707] 310.2 vec/sec, ETA: 0.0 min
    [480/707] 314.0 vec/sec, ETA: 0.0 min
    [490/707] 314.2 vec/sec, ETA: 0.0 min
    [500/707] 315.1 vec/sec, ETA: 0.0 min
    [510/707] 317.4 vec/sec, ETA: 0.0 min
    [520/707] 320.4 vec/sec, ETA: 0.0 min
    [530/707] 320.3 vec/sec, ETA: 0.0 min
    [540/707] 323.2 vec/sec, ETA: 0.0 min
    [550/707] 323.0 vec/sec, ETA: 0.0 min
    [560/707] 324.3 vec/sec, ETA: 0.0 min
    [570/707] 325.7 vec/sec, ETA: 0.0 min
    [580/707] 328.4 vec/sec, ETA: 0.0 min
    [590/707] 331.2 vec/sec, ETA: 0.0 min
    [600/707] 330.9 vec/sec, ETA: 0.0 min
    [610/707] 330.5 vec/sec, ETA: 0.0 min
    [620/707] 333.1 vec/sec, ETA: 0.0 min
    [630/707] 335.6 vec/sec, ETA: 0.0 min
    [640/707] 335.3 vec/sec, ETA: 0.0 min
    [650/707] 337.7 vec/sec, ETA: 0.0 min
    [660/707] 337.4 vec/sec, ETA: 0.0 min
    [670/707] 339.8 vec/sec, ETA: 0.0 min
    [680/707] 339.4 vec/sec, ETA: 0.0 min
    [690/707] 341.7 vec/sec, ETA: 0.0 min
    [700/707] 341.3 vec/sec, ETA: 0.0 min
[+] Reconstruction complete in 2.1s
[+] Statistics:
    Total coefficients: 1831130
    Zero coefficients: 1751993 (95.7%)
    Reconstructed: 79137
    Failed: 0
    Verification OK: 78472
    Verification FAIL: 665
[+] Wrote rational basis to kernel_basis_Q.json
[+] Wrote failures to validator_v2\reconstruction_failures.json
ERROR: 665 coefficients failed verification
       DO NOT USE THE BASIS until failures are resolved.

c:\math>python reconstruct_failures.py
Total failures: 665
{'vec': 20, 'coeff': 52, 'residues': [52, 78, 130, 124, 23], 'n': -1, 'd': 1, 'note': 'verification_failed_mod_157'}
{'vec': 20, 'coeff': 173, 'residues': [27, 61, 45, 57, 158], 'n': 1359, 'd': 280, 'note': 'verification_failed_mod_131'}
{'vec': 20, 'coeff': 216, 'residues': [13, 64, 128, 40, 176], 'n': -256, 'd': 653, 'note': 'verification_failed_mod_79'}
{'vec': 20, 'coeff': 413, 'residues': [8, 63, 32, 96, 34], 'n': 301, 'd': 865, 'note': 'verification_failed_mod_53'}
{'vec': 20, 'coeff': 422, 'residues': [2, 55, 69, 57, 210], 'n': 364, 'd': 235, 'note': 'verification_failed_mod_313'}
{'vec': 20, 'coeff': 441, 'residues': [24, 67, 102, 58, 294], 'n': -779, 'd': 980, 'note': 'verification_failed_mod_53'}
{'vec': 20, 'coeff': 443, 'residues': [13, 70, 66, 26, 215], 'n': 893, 'd': 1393, 'note': 'verification_failed_mod_53'}
{'vec': 20, 'coeff': 462, 'residues': [5, 78, 54, 62, 306], 'n': -1226, 'd': 41, 'note': 'verification_failed_mod_131'}
{'vec': 20, 'coeff': 577, 'residues': [11, 37, 59, 42, 219], 'n': 1764, 'd': 827, 'note': 'verification_failed_mod_53'}
{'vec': 20, 'coeff': 583, 'residues': [1, 1, 1, 85, 199], 'n': 1, 'd': 1, 'note': 'verification_failed_mod_157'}
{'vec': 20, 'coeff': 594, 'residues': [11, 16, 25, 48, 119], 'n': -2861, 'd': 310, 'note': 'verification_failed_mod_53'}
{'vec': 20, 'coeff': 640, 'residues': [16, 26, 116, 89, 288], 'n': -33, 'd': 1679, 'note': 'verification_failed_mod_53'}
{'vec': 20, 'coeff': 725, 'residues': [41, 64, 103, 90, 251], 'n': 271, 'd': 556, 'note': 'verification_failed_mod_131'}
{'vec': 20, 'coeff': 919, 'residues': [43, 8, 80, 57, 273], 'n': 1211, 'd': 181, 'note': 'verification_failed_mod_131'}
{'vec': 21, 'coeff': 193, 'residues': [5, 34, 103, 14, 118], 'n': 1662, 'd': 343, 'note': 'verification_failed_mod_79'}
{'vec': 21, 'coeff': 299, 'residues': [23, 33, 98, 72, 35], 'n': 1990, 'd': 817, 'note': 'verification_failed_mod_79'}
{'vec': 21, 'coeff': 583, 'residues': [12, 77, 28, 113, 279], 'n': 843, 'd': 905, 'note': 'verification_failed_mod_79'}
{'vec': 21, 'coeff': 635, 'residues': [35, 31, 108, 74, 145], 'n': 1299, 'd': 758, 'note': 'verification_failed_mod_53'}
{'vec': 22, 'coeff': 162, 'residues': [40, 67, 83, 97, 260], 'n': -356, 'd': 1241, 'note': 'verification_failed_mod_53'}
{'vec': 22, 'coeff': 556, 'residues': [13, 19, 41, 30, 192], 'n': -492, 'd': 643, 'note': 'verification_failed_mod_79'}

c:\math>python debug_reconstruct.py
vec,coeff: 20 52
residues: [(53, 52), (79, 78), (131, 130), (157, 124), (313, 23)]
CRT cM: 24743248166 M: 26953691077
mult 1 bound 116089 -> (-1, 1)
  congruence ok? False
mult 2 bound 232178 -> (-1, 1)
  congruence ok? False
mult 4 bound 464356 -> (-1, 1)
  congruence ok? False
mult 8 bound 928712 -> (-550070, 1573)
  congruence ok? True
```

Therefore I needed to create more json invariant files for monomial and triplets, so I needed to use this m2 script from qualia_candidate_axioms/potential_hodge_conjecture_counterexample.md and change the primes to 443, 521, 547, 599, 677, and 911:

```m2
-- verify_invariant_tier2.m2  (fixed)
-- verify_invariant_tier2.m2 (streamed-triplet safe version)
-- TIER II: Symmetry Obstruction via C13-Invariant Sector
-- Writes monomials and triplets in streaming fashion to avoid large in-memory JSON
needsPackage "JSON";

-- CONFIGURATION: edit this list to include additional primes (all p ‚â° 1 mod 13)
primesToTest = {443, 521, 547, 599, 677, 911};

print("=== TIER II: Symmetry Obstruction Verification (safe IO) ===");

for p in primesToTest do (
    if (p % 13) != 1 then continue;

    print("--- Prime: " | toString p | " ---");

    -- 1. Setup finite field with 13th root
    Fp := ZZ/p;
    w := 0_Fp;
    for a from 2 to p-1 do (
        cand := (a * 1_Fp)^((p-1)//13);
        if (cand != 1_Fp) and (cand^13 == 1_Fp) then (
            w = cand;
            break;
        );
    );
    print("Using 13th root w = " | toString w);

    -- 2. Build polynomial ring
    S := Fp[z_0..z_5];
    z := gens S;

    -- 3. Construct C13-invariant variety
    print("Assembling C13-invariant variety...");
    linearForms := for k from 0 to 12 list (
        sum(0..5, j -> (w^((k*j) % 13)) * z#j)
    );
    fS := sum(linearForms, l -> l^8);

    -- 4. Compute partial derivatives
    partials := for i from 0 to 5 list diff(z#i, fS);

    -- 5. Generate invariant monomial basis
    print("Generating degree-18 invariant monomials...");
    mon18List := flatten entries basis(18, S);

    -- Filter to invariant:   Œ£ j¬∑a_j ‚â° 0 (mod 13)
    invMon18 := select(mon18List, m -> (
        ev := (exponents m)#0;
        (sum(for j from 0 to 5 list j * ev#j)) % 13 == 0
    ));

    countInv := #invMon18;
    print("Invariant monomials (deg 18): " | toString countInv);

    -- 6. Build index map (use string keys)
    print("Building index map...");
    monToIndex := new MutableHashTable;
    for i from 0 to countInv - 1 do (
        monToIndex#(toString(invMon18#i)) = i;
    );

    -- 7. Filter Jacobian generators (character matching)
    print("Filtering Jacobian generators...");
    mon11List := flatten entries basis(11, S);

    filteredGens := {};
    for i from 0 to 5 do (
        targetWeight := i;
        for m in mon11List do (
            mWeight := (sum(for j from 0 to 5 list j * (exponents m)#0#j)) % 13;
            if mWeight == targetWeight then (
                filteredGens = append(filteredGens, m * partials#i);
            );
        );
    );

    print("Filtered generators: " | toString(#filteredGens));

    -- 8. Assemble coefficient matrix (mutable)
    print("Assembling matrix (MutableMatrix)...");
    M := mutableMatrix(Fp, countInv, #filteredGens);

    for j from 0 to (#filteredGens - 1) do (
        (mons, coeffs) := coefficients filteredGens#j;
        mList := flatten entries mons;
        cList := flatten entries coeffs;
        for k from 0 to (#mList - 1) do (
            m := mList#k;
            key := toString m;
            if monToIndex #? key then (
                M_(monToIndex#key, j) = sub(cList#k, Fp);
            );
        );
    );

    MatInv := matrix M;

    -- 9. Compute rank
    print("Computing rank (symmetry-locked)...");
    time rk := rank MatInv;
    h22inv := countInv - rk;

    -- 10. Results summary
    print("----------------------------------------");
    print("RESULTS FOR PRIME " | toString p | ":");
    print("Invariant monomials: " | toString countInv);
    print("Rank: " | toString rk);
    print("h^{2,2}_inv: " | toString h22inv);
    print("Gap (h22_inv - 12 algebraic): " | toString(h22inv - 12));
    print("Gap percentage: " | toString(100.0 * (h22inv - 12) / h22inv) | "%");
    print("----------------------------------------");

    -- 11. Export monomials (safe small JSON)
    monFile := "validator_v2/saved_inv_p" | toString p | "_monomials18.json";
    print("Writing monomials to " | monFile);
    monExps := for m in invMon18 list (
        ev := (exponents m)#0;
        for j from 0 to 5 list ev#j
    );
    (openOut monFile) << toJSON(monExps) << close;

    -- 12. Export triplets in streaming fashion to avoid giant memory use
    triFile := "validator_v2/saved_inv_p" | toString p | "_triplets.json";
    print("Streaming triplets to " | triFile);
    out = openOut triFile;

    -- Start JSON object (write header)
    out << "{\n";
    out << "  \"prime\": " | toString p | ",\n";
    out << "  \"h22_inv\": " | toString h22inv | ",\n";
    out << "  \"rank\": " | toString rk | ",\n";
    out << "  \"countInv\": " | toString countInv | ",\n";
    out << "  \"triplets\": [\n";

    firstEntry = true;
    nC := numColumns MatInv;
    nR := numRows MatInv;
    -- iterate columns then rows to stream nonzeros
    for c from 0 to (nC - 1) do (
        for r from 0 to (nR - 1) do (
            val := MatInv_(r,c);
            if val != 0_Fp then (
                if firstEntry then (
                    firstEntry = false;
                ) else (
                    out << ",\n";
                );
                -- write JSON array [r, c, integer_value]
                out << "    [" | toString r | ", " | toString c | ", " | toString lift(val, ZZ) | "]";
            );
        );
    );

    -- Close JSON array and object
    out << "\n  ]\n}\n";
    close out;

    -- 13. Null references to help M2 free memory
    MatInv = 0;
    M = 0;
    filteredGens = {};
    mon18List = {};
    invMon18 = {};
    monToIndex = new MutableHashTable;
);

print("=== TIER II Verification Complete ===");
```

then convert those jsons into the proper kernel files! I am putting all invariant files into its own folder in validator_v2 however the kernels are too big and must be generated by the user! You must generate the kernel from the invariants yourself one by one to fulfill the process, as many times with as many primes as it takes. Hopefully the amount already generated here will be enough, we will see!
---

**END OF ARTIFACT**
