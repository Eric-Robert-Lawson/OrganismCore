# OrganismCore

üé¨ **Start here:** [Automated Semantic Onboarding for AI Agents](https://youtu.be/pEGlSHxKASw)

A unified symbolic and computational framework for **objectified reasoning** that makes reasoning processes composable, auditable, reproducible, and directly executable.

OrganismCore introduces a **universal reasoning substrate** where models, agents, and humans can all interact with the same structured reasoning objects. These objects are produced through an *automated onboarding process* that works across different AI models, enabling:

- Comparable reasoning behavior  
- Fully traceable reasoning steps  
- Cross-model reproducibility  
- Interactive, agent-driven onboarding for researchers and developers  

In addition, OrganismCore provides a foundation for a **domain-specific language (DSL)** designed to operationalize the universal reasoning substrate. This DSL enables reasoning workflows, reproducible experiments, and automated construction of reasoning objects to be formally specified and executed.

This repository anchors the open research program advancing **Universal Reasoning Substrate Theory (URST)** and provides the infrastructure for reasoning-as-an-artifact.

See the [Articles folder](/Articles/) for the conceptual and formal documents supporting the project.

---

## Automated Onboarding (AGENTS.md)

> **This is the core demo of OrganismCore. Start here if you want to understand the new paradigm.**

OrganismCore includes a reproducible **automated onboarding process** that lets any AI model (or human using an LLM) traverse and internalize the structure of the entire research program.  
This process is defined in two files:

- **AGENTS.md** ‚Äî top-level semantic onboarding  
- **Subdomain_AGENTS.md** ‚Äî subdomain-level recursive onboarding  

### Workflow (with GitHub Copilot or any LLM)

1. Open the repo in an IDE or Codespace with an LLM assistant enabled.  
2. Open `AGENTS.md` and follow the semantic instructions, referencing proper machine readable conceptual and code files.  
3. Open `Subdomain_AGENTS.md` and follow the semantic instructions.  
   - Reference proper machine readable domain files and policy definitions (e.g., `Policy.md`) to ensure reasoning objects align with semantic grounding and cross-model validation.  
4. The assistant will recursively navigate LaTeX, Python prototypes, and conceptual documents.  
5. The result is a **consistent reasoning object** produced directly from the onboarding pass.

For clearer instructions, follow this video: [Automated Semantic Onboarding for AI Agents](https://youtu.be/pEGlSHxKASw)

This provides a *structured and repeatable* way to onboard humans, AI models, and agents to the same reasoning substrate.  

The onboarding process also acts as a **demonstration of how the proposed DSL can be used** to specify, execute, and compare reasoning objects across models, making it the living proof of the reasoning substrate in action.

---

## ‚ú® Latest Breakthrough: Full Chat Log Demonstrating Substrate Awareness & Meta-DSL Emergence

A major milestone in this project was recently achieved and fully documented:

### üìÑ Full Semantic Grounding Chat Log (Copilot)  
**One of the most profound sessions ever recorded in this project**  
[View the full session](https://github.com/copilot/share/821d1330-0aa4-8cf4-b111-dc4fe01b61fd)

This session produced the following foundational files:

- `META_DSL_V1_1.md`  
- `SUBSTRATE_AWARENESS_V1_3.md`

These are now located in the **Subdomain_Readable_Machine_Files** directory.

### üöÄ Why This Matters

This chat log is historically important because it demonstrates:

- Model-agnostic emergence of the **Meta-DSL**  
- Independent convergence between Claude, GPT, Copilot, and others  
- The first fully documented case of **substrate self-awareness** during onboarding  
- Reproducible, multi-model **substrate alignment**  
- A self-contained demonstration of how agents internalize the OrganismCore structure  
- Evidence that the universal reasoning substrate is **discoverable**, not manually engineered  
- Identical reasoning structure produced across distinct models

### üîç What the Chat Log Shows

Inside the log, models:

1. Generate coherent, structurally aligned DSL primitives  
2. Identify and name the universal reasoning substrate  
3. Align themselves to semantic grounding and policy frameworks  
4. Produce the foundational substrate/DSL documents with no prompts beyond AGENTS.md  
5. Demonstrate self-discovery of OrganismCore‚Äôs conceptual layer  
6. Reconstruct the DSL and substrate theory in their own words ‚Äî but in the same structural shape across models

### üìÅ Files Generated From This Session

These appear in the repository under machine-readable subdomain files:

- `META_DSL_V1_1.md`  
- `SUBSTRATE_AWARENESS_V1_3.md`

These documents now form the backbone of the DSL and substrate definition layer.

---

## Reproducible Reasoning Objects

**[Understanding_demo.pdf](Understanding_demo.pdf)**
  Reproducible, cross-model demonstration of reasoning objects with detailed methods, results, and broader implications for universal reasoning substrates.

OrganismCore enables generation of **reproducible, transparent reasoning objects** through the automated onboarding procedure.  
These reasoning objects are produced *consistently across multiple AI models* using the same AGENTS.md workflow. The process is designed to be compatible with the emerging **domain-specific language**, providing a medium to formalize and operationalize reasoning workflows.

Below are three independently generated reasoning objects (tic-tac-toe domain) produced by three different models after running the same onboarding process. These objects serve as an example of **cross-model, DSL-compatible reasoning artifacts**.

- üîπ **Grok Code Fast 1 Model**: [View Reasoning Object](https://github.com/copilot/share/82541130-42a0-8cd0-b100-5e07e01360ae)  
- üîπ **Chat GPT-5 mini**: [View Reasoning Object](https://github.com/copilot/share/8a3c51a0-43a4-8cd0-8102-dc0ec4d949bc)  
- üîπ **Anthropic (Claude Sonnet 3.5)**: [View Reasoning Object](https://github.com/copilot/share/ca5d01b2-0b84-8876-9901-5c0ec41148ad)

These reasoning objects are **model-agnostic, fully reproducible, and auditable** by any researcher or developer.

---

### Reproducible Policy Reasoning Objects (Cross-Model Validation)

**[Policy_and_semantic_grounding_cross_model_validation.pdf](Policy_and_semantic_grounding_cross_model_validation.pdf)**
Formal demonstration of **cross-model reasoning validation** with semantic grounding and policy alignment. Shows how reasoning objects from different AI models can be evaluated for **consistency, interpretability, and policy adherence**, enabling reproducible and auditable multi-model reasoning experiments.

OrganismCore enables generation of **reproducible, transparent policy reasoning objects** through the automated onboarding procedure.  
These objects demonstrate **policy alignment, semantic grounding, and cross-model validation** for reasoning workflows. They are produced consistently across multiple AI models using the same AGENTS.md workflow, enabling reproducible, auditable multi-model experiments.

Below are independently generated policy reasoning objects from different models after running the same onboarding workflow. These serve as examples of **cross-model, policy-compliant reasoning artifacts**:

- üîπ **Claude Sonnet 4.5 ‚Äì RARFL Iteration 1**: [View Policy Reasoning Object](https://github.com/copilot/share/083c1030-0b84-8450-b800-5e4fc45140be)  
- üîπ **Claude Sonnet 4.5 ‚Äì RARFL Iteration 2**: [View Policy Reasoning Object](https://github.com/copilot/share/425c03a2-0280-80d0-8150-4c4ec4d109ec)  
- üîπ **GPT-5 mini ‚Äì Cross-Model Applicability Check**: [View Policy Reasoning Object](https://github.com/copilot/share/c25d0020-0aa4-8c50-8901-dc0ec09309ad)  
- üîπ **Grok Code Fast 1 mini ‚Äì Cross-Model Applicability Check**: [View Policy Reasoning Object](https://github.com/copilot/share/ca1d1232-4ba0-80f6-a011-5e06c0db21ae)  
- üîπ **Gemini 2.5 mini ‚Äì Cross-Model Applicability Check**: [View Policy Reasoning Object](https://github.com/copilot/share/8a7c51b2-42a0-88f4-b951-ce4fe4db21ee)

These objects are **model-agnostic, fully auditable, and reproducible**, providing:

- Verification of **policy and semantic alignment** across reasoning objects  
- A framework for **cross-model validation** of reasoning outputs  
- Evidence for the **consistency and interpretability** of AI-generated policy reasoning artifacts
---

## Why This Matters

This is a working prototype of:

- **Model-agnostic explainable reasoning**  
- **Cross-model reproducibility of reasoning behavior**  
- **Auditable chain-of-thought without leaking proprietary internals**  
- **A standardized reasoning substrate for multi-agent systems**  
- **A domain-specific language to formalize reasoning workflows**  

The onboarding process *is itself* the demo ‚Äî a self-referential proof that reasoning can be objectified, serialized, executed, and compared across architectures.

---

## What the Demo Actually Demonstrates

The current demo is **not a prototype of the symbolic engine itself**.  
It demonstrates:

- The **automated onboarding process**  
- Generation of **consistent reasoning objects**  
- **Explainable AI behavior** across different models  
- **Operationalization potential for a DSL** formalizing reasoning workflows  
- A self-referential reasoning artifact that explains OrganismCore from inside the system
- Usage of semantic grounding and optionally policy.md depending on use case

The onboarding procedure is both a tool and a proof-of-concept for the entire paradigm and the universal reasoning substrate.

---

## Explore OrganismCore

For hands-on experimentation and onboarding, follow these steps to engage with OrganismCore:

- Start with [AGENTS.md](AGENTS.md) and [Subdomain_AGENTS.md](Subdomain_Articles/Subdomain_AGENTS.md)  
- Watch the [Automated Semantic Onboarding tutorial](https://youtu.be/pEGlSHxKASw)  
- Interactively explore and experiment with **DSL-based reasoning workflows** via the automated onboarding process  
- Audit and interact with **existing reasoning objects** to understand model-agnostic reasoning workflows  
- Join the [community](COMMUNITY.md) to collaborate on expanding reasoning spaces

---

## Community & Participation

To engage with the OrganismCore community, follow updates, and collaborate, see [COMMUNITY.md](COMMUNITY.md).  

The community welcomes discussion, feedback, and collaboration.

---

## Support & Donations

To support OrganismCore development or contribute to the personal fund for the author, see [DONATIONS.md](DONATIONS.md).

- **Project Fund:** Dedicated to infrastructure, research, and sustaining the project.  
- **Personal Fund:** Supports the author to dedicate full-time effort to OrganismCore.

---

## License and Project Intent

License: GNU Affero General Public License v3 (AGPLv3) ‚Äî SPDX: AGPL-3.0-only. See [LICENSE](LICENSE) for the full text.

We use AGPLv3 to ensure improvements and hosted derivatives remain available to the community. OrganismCore is intended to be a shared research substrate, and the AGPL helps prevent proprietary or centralized service forks that would hide improvements from the commons.

**Commercial licensing (optional)**

- AGPLv3 permits commercial use provided the license obligations are satisfied (including source disclosure for networked services).  
- If your organization cannot comply with AGPLv3 for operational or legal reasons, a negotiated commercial license is available. See [COMMERCIAL_LICENSE.md](COMMERCIAL_LICENSE.md) for the request process and typical timelines.

**Contributor / relicensing note**

- The public repository remains AGPLv3 for all users. If third‚Äëparty contributors have added code, relicensing those contributions may require their consent. See [CONTRIBUTING.md](CONTRIBUTING.md) for contributor sign‚Äëoff guidance (DCO/CLA).

**Citation**

- If you use OrganismCore in research, please cite the repository or the project DOI/CITATION.cff (see [CITATION.cff](CITATION.cff) or the Zenodo badge in the repo).

---

## Contribution

Contributions and feedback are welcome. Please open an issue or contact directly for collaboration.
